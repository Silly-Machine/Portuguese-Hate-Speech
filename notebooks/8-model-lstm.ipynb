{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unable warnings\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directory adjustment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Back to main folder\n",
    "path = os.path.dirname(os.getcwd()) + \"/\"\n",
    "os.chdir(path)\n",
    "sys.path.append(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural language processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim import models\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models\n",
    "[Check](https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML preprocessing\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Deep learnig model\n",
    "#from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Train  metrics\n",
    "METRICS = [\n",
    "    tf.keras.metrics.TruePositives(name='tp'),\n",
    "    tf.keras.metrics.FalsePositives(name='fp'),\n",
    "    tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "    tf.keras.metrics.FalseNegatives(name='fn'),\n",
    "    tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall'),\n",
    "    tf.keras.metrics.AUC(name='auc'),\n",
    "    tf.keras.metrics.AUC(name='prc', curve='PR'),  # precision-recall curve\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='./artifacts/1', creation_time=1665929754799, experiment_id='1', last_update_time=1665929754799, lifecycle_stage='active', name='Hate Speech', tags={}>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri('http://127.0.0.1:5000')\n",
    "mlflow.set_experiment('Hate Speech')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "df = pd.read_csv(\"data/corpus/augmented_corpus_fortuna.csv\")\n",
    "\n",
    "# Set target and features\n",
    "target = \"label\"\n",
    "features = \"text_nonstop\"\n",
    "count = f\"length_{features}\"\n",
    "pos = len(df.query('label==1'))\n",
    "neg = len(df.query('label==0'))\n",
    "\n",
    "\n",
    "# Break apart dataset\n",
    "X = df[features].values.astype(\"U\")\n",
    "y = df[target]\n",
    "\n",
    "# Split train abd test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Set k-fold criteria\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Classes balancing\n",
    "longest_text = df[count].max()\n",
    "initial_bias = np.log([pos/neg])\n",
    "\n",
    "weight_for_0 = (1 / neg) * (len(df) / 2.0)\n",
    "weight_for_1 = (1 / pos) * (len(df) / 2.0)\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding\n",
    "w2v = KeyedVectors.load_word2vec_format(\n",
    "    \"data/pretrained-glove/glove_s50.txt\", binary=False\n",
    ")\n",
    "\n",
    "# Embedding props\n",
    "vocab_size = len(w2v) + 1\n",
    "vec_dim = w2v.vectors.shape[1]\n",
    "embedding_weights = np.vstack([\n",
    "    np.zeros(w2v.vectors.shape[1]),\n",
    "    w2v.vectors\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerTransformer(BaseEstimator, TransformerMixin, Tokenizer):\n",
    "    def __init__(self, **tokenizer_params):\n",
    "        Tokenizer.__init__(self, **tokenizer_params)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.fit_on_texts(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = self.texts_to_sequences(X)\n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "class PadSequencesTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, maxlen):\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_padded = pad_sequences(X, maxlen=self.maxlen)\n",
    "        return X_padded\n",
    "\n",
    "class ModelWrapper(mlflow.pyfunc.PythonModel): \n",
    "    def __init__(self, model): \n",
    "        self.model = model \n",
    " \n",
    "    def predict(self, context, model_input): \n",
    "        return self.model.predict(model_input) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preset parameters\n",
    "experiment_parameters = {\"classifier\": \"LSTM\",\n",
    "                         \"class_weight\": class_weight,\n",
    "                         \"epochs\": 15,\n",
    "                         \"units\": 50,\n",
    "                         \"dropout\": 0.4,\n",
    "                         \"recurrent_dropout\": 0.2,\n",
    "                         \"kernel_initializer\": 'glorot_uniform',\n",
    "                         \"loss\": \"binary_crossentropy\",\n",
    "                         \"optimizer\": \"adamax\",\n",
    "                         \"embedding_input_dim\": vec_dim,\n",
    "                         \"batch_size\": 64}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model\n",
    "\n",
    "def lstm_builder(embedding_input_dim, embedding_output_dim, embedding_weights):\n",
    "    output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "    lstm = Sequential()\n",
    "\n",
    "    lstm.add(\n",
    "        Embedding(\n",
    "            input_dim=embedding_input_dim,\n",
    "            output_dim=embedding_output_dim,\n",
    "            weights=[embedding_weights],\n",
    "            trainable=False,\n",
    "            mask_zero=True,\n",
    "        )\n",
    "    )\n",
    "    lstm.add(Bidirectional(LSTM(units=experiment_parameters['units'],\n",
    "                                dropout=experiment_parameters['dropout'],\n",
    "                                recurrent_dropout=experiment_parameters['recurrent_dropout'],\n",
    "                                kernel_initializer=experiment_parameters['kernel_initializer'])))\n",
    "\n",
    "    lstm.add(Dropout(0.20))\n",
    "\n",
    "    lstm.add(Dense(units=1,\n",
    "                   activation=\"sigmoid\",\n",
    "                   bias_initializer=output_bias))\n",
    "\n",
    "    lstm.compile(loss=experiment_parameters['loss'],\n",
    "                 optimizer=experiment_parameters['optimizer'],\n",
    "                 metrics=METRICS)\n",
    "    return lstm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model execution\n",
    "lstm = KerasClassifier(\n",
    "    model=lstm_builder,\n",
    "    epochs=experiment_parameters['epochs'],\n",
    "    embedding_input_dim=len(w2v) + 1,\n",
    "    embedding_output_dim=vec_dim,\n",
    "    embedding_weights=embedding_weights,\n",
    "    batch_size=experiment_parameters['batch_size'],\n",
    "    callbacks=[EarlyStopping(monitor=\"loss\",\n",
    "                             patience=10,\n",
    "                             restore_best_weights=True)],\n",
    "    class_weight=class_weight\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/10/19 14:41:34 WARNING mlflow.utils: Truncated the value of the key `steps`. Truncated value: `[('tokenizer', TokenizerTransformer()), ('padder', PadSequencesTransformer(maxlen=131)), ('model', KerasClassifier(\n",
      "\tmodel=<function lstm_builder at 0x7fa461dbf940>\n",
      "\tbuild_fn=None\n",
      "\twarm_start=False\n",
      "\trandom_state=None\n",
      "\toptimizer=rmsprop\n",
      "\tloss=None\n",
      "\tmetrics=None\n",
      "\tbatch_size=64\n",
      "\tvalidation_batch_size=None\n",
      "\tverbose=1\n",
      "\tcallbacks=[<keras.callbacks.EarlyStopping object at 0x7fa463b56fd0>]\n",
      "\tvalidation_split=0.0\n",
      "\tshuffle=True\n",
      "\trun_eagerly=False\n",
      "\tepochs=15\n",
      "\tembedding_input_dim=929606\n",
      "\tembedding_output_...`\n",
      "2022/10/19 14:41:34 WARNING mlflow.utils: Truncated the value of the key `model`. Truncated value: `KerasClassifier(\n",
      "\tmodel=<function lstm_builder at 0x7fa461dbf940>\n",
      "\tbuild_fn=None\n",
      "\twarm_start=False\n",
      "\trandom_state=None\n",
      "\toptimizer=rmsprop\n",
      "\tloss=None\n",
      "\tmetrics=None\n",
      "\tbatch_size=64\n",
      "\tvalidation_batch_size=None\n",
      "\tverbose=1\n",
      "\tcallbacks=[<keras.callbacks.EarlyStopping object at 0x7fa463b56fd0>]\n",
      "\tvalidation_split=0.0\n",
      "\tshuffle=True\n",
      "\trun_eagerly=False\n",
      "\tepochs=15\n",
      "\tembedding_input_dim=929606\n",
      "\tembedding_output_dim=50\n",
      "\tembedding_weights=[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      "...`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "71/71 [==============================] - 13s 120ms/step - loss: 0.7063 - tp: 1592.0000 - fp: 2369.0000 - tn: 3845.0000 - fn: 1266.0000 - accuracy: 0.5993 - precision: 0.4019 - recall: 0.5570 - auc: 0.6203 - prc: 0.4380\n",
      "Epoch 2/15\n",
      "71/71 [==============================] - 9s 124ms/step - loss: 0.6817 - tp: 873.0000 - fp: 1492.0000 - tn: 1615.0000 - fn: 556.0000 - accuracy: 0.5485 - precision: 0.3691 - recall: 0.6109 - auc: 0.5853 - prc: 0.3841\n",
      "Epoch 3/15\n",
      "71/71 [==============================] - 9s 121ms/step - loss: 0.6731 - tp: 868.0000 - fp: 1409.0000 - tn: 1698.0000 - fn: 561.0000 - accuracy: 0.5657 - precision: 0.3812 - recall: 0.6074 - auc: 0.6128 - prc: 0.4127\n",
      "Epoch 4/15\n",
      "71/71 [==============================] - 8s 118ms/step - loss: 0.6722 - tp: 845.0000 - fp: 1318.0000 - tn: 1789.0000 - fn: 584.0000 - accuracy: 0.5807 - precision: 0.3907 - recall: 0.5913 - auc: 0.6163 - prc: 0.4156\n",
      "Epoch 5/15\n",
      "71/71 [==============================] - 8s 117ms/step - loss: 0.6733 - tp: 855.0000 - fp: 1342.0000 - tn: 1765.0000 - fn: 574.0000 - accuracy: 0.5776 - precision: 0.3892 - recall: 0.5983 - auc: 0.6143 - prc: 0.4125\n",
      "Epoch 6/15\n",
      "71/71 [==============================] - 9s 121ms/step - loss: 0.6677 - tp: 815.0000 - fp: 1228.0000 - tn: 1879.0000 - fn: 614.0000 - accuracy: 0.5939 - precision: 0.3989 - recall: 0.5703 - auc: 0.6269 - prc: 0.4323\n",
      "Epoch 7/15\n",
      "71/71 [==============================] - 8s 119ms/step - loss: 0.6646 - tp: 855.0000 - fp: 1278.0000 - tn: 1829.0000 - fn: 574.0000 - accuracy: 0.5917 - precision: 0.4008 - recall: 0.5983 - auc: 0.6350 - prc: 0.4338\n",
      "Epoch 8/15\n",
      "71/71 [==============================] - 8s 119ms/step - loss: 0.6638 - tp: 835.0000 - fp: 1204.0000 - tn: 1903.0000 - fn: 594.0000 - accuracy: 0.6036 - precision: 0.4095 - recall: 0.5843 - auc: 0.6336 - prc: 0.4461\n",
      "Epoch 9/15\n",
      "71/71 [==============================] - 8s 119ms/step - loss: 0.6589 - tp: 858.0000 - fp: 1174.0000 - tn: 1933.0000 - fn: 571.0000 - accuracy: 0.6153 - precision: 0.4222 - recall: 0.6004 - auc: 0.6474 - prc: 0.4539\n",
      "Epoch 10/15\n",
      "71/71 [==============================] - 8s 117ms/step - loss: 0.6618 - tp: 833.0000 - fp: 1193.0000 - tn: 1914.0000 - fn: 596.0000 - accuracy: 0.6056 - precision: 0.4112 - recall: 0.5829 - auc: 0.6422 - prc: 0.4414\n",
      "Epoch 11/15\n",
      "71/71 [==============================] - 8s 117ms/step - loss: 0.6599 - tp: 811.0000 - fp: 1132.0000 - tn: 1975.0000 - fn: 618.0000 - accuracy: 0.6142 - precision: 0.4174 - recall: 0.5675 - auc: 0.6465 - prc: 0.4456\n",
      "Epoch 12/15\n",
      "71/71 [==============================] - 9s 132ms/step - loss: 0.6573 - tp: 850.0000 - fp: 1132.0000 - tn: 1975.0000 - fn: 579.0000 - accuracy: 0.6228 - precision: 0.4289 - recall: 0.5948 - auc: 0.6514 - prc: 0.4556\n",
      "Epoch 13/15\n",
      "71/71 [==============================] - 9s 126ms/step - loss: 0.6537 - tp: 860.0000 - fp: 1154.0000 - tn: 1953.0000 - fn: 569.0000 - accuracy: 0.6201 - precision: 0.4270 - recall: 0.6018 - auc: 0.6596 - prc: 0.4648\n",
      "Epoch 14/15\n",
      "71/71 [==============================] - 9s 121ms/step - loss: 0.6520 - tp: 859.0000 - fp: 1122.0000 - tn: 1985.0000 - fn: 570.0000 - accuracy: 0.6270 - precision: 0.4336 - recall: 0.6011 - auc: 0.6624 - prc: 0.4708\n",
      "Epoch 15/15\n",
      "71/71 [==============================] - 9s 121ms/step - loss: 0.6512 - tp: 864.0000 - fp: 1155.0000 - tn: 1952.0000 - fn: 565.0000 - accuracy: 0.6208 - precision: 0.4279 - recall: 0.6046 - auc: 0.6625 - prc: 0.4736\n",
      "71/71 [==============================] - 2s 19ms/step\n",
      "71/71 [==============================] - 1s 19ms/step\n",
      "71/71 [==============================] - 1s 19ms/step\n",
      "71/71 [==============================] - 1s 19ms/step\n",
      "71/71 [==============================] - 1s 19ms/step\n",
      "71/71 [==============================] - 2s 21ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///tmp/tmp1idr1qb7/assets\n",
      "18/18 [==============================] - 0s 18ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///tmp/tmpykrdtk3o/assets\n"
     ]
    }
   ],
   "source": [
    "mlflow.sklearn.autolog()\n",
    "with mlflow.start_run():\n",
    "\n",
    "    ml_pipe = Pipeline(\n",
    "        [(\"tokenizer\",  TokenizerTransformer()),\n",
    "         (\"padder\", PadSequencesTransformer(maxlen=longest_text)),\n",
    "         (\"model\", lstm)])\n",
    "\n",
    "    # Model fit\n",
    "    ml_pipe.fit(X_train, y_train)\n",
    "    y_predict = ml_pipe.predict(X_test)\n",
    "\n",
    "    # Tracking\n",
    "    mlflow.log_metric('precision', precision_score(y_test, y_predict))\n",
    "    mlflow.log_metric('accuracy', accuracy_score(y_test, y_predict))\n",
    "    mlflow.log_metric('recall', recall_score(y_test, y_predict))\n",
    "    mlflow.log_metric('auc', roc_auc_score(y_test, y_predict))\n",
    "    mlflow.log_metric('f1', f1_score(y_test, y_predict))\n",
    "    mlflow.pyfunc.log_model(\n",
    "        python_model=ModelWrapper(ml_pipe),\n",
    "        artifact_path=\"LSTM\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('hate-seepch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7304d3c1a35396b9e299acc644b89f0e56d4154029b20ed6ab08effb23ac070c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
