{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unable warnings\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directory adjustment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "# Back to main folder\n",
    "path = os.path.dirname(os.getcwd())+\"/\"\n",
    "os.chdir(path)\n",
    "sys.path.append(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Charts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG, display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# ML preprocessing\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural language processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing\n",
    "from nltk.stem.snowball import PortugueseStemmer\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from cleantext import clean\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "# Vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking\n",
    "import mlflow\n",
    "# Pipe\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm\n",
    "# Metrics\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set and split train and test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "df_train = pd.read_csv('data/corpus/train_data.csv')\n",
    "df_test = pd.read_csv('data/corpus/test_data.csv')\n",
    "\n",
    "# Set target and features\n",
    "target = 'label'\n",
    "features = 'text'\n",
    "\n",
    "# Set train and test\n",
    "X_train, y_train = df_train[features], df_train[target]\n",
    "X_test, y_test = df_test[features], df_test[target]\n",
    "\n",
    "\n",
    "# Class weights\n",
    "pos = len(df_train.query('label==1'))\n",
    "neg = len(df_train.query('label==0'))\n",
    "extra = 1.25\n",
    "weight_for_0 = (1 / neg) * (len(df_train) / 2.0)\n",
    "weight_for_1 = (1 / pos) * (len(df_train) / 2.0)*extra\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "\n",
    "# Set k-fold criteria\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalizer(text):\n",
    "    text = re.sub(r\"@[^\\s]+\", \"nome_usuario\", text)\n",
    "    text = clean(\n",
    "        text,\n",
    "        fix_unicode=True, to_ascii=True,\n",
    "        lower=True, no_emoji=True,\n",
    "        no_line_breaks=True, no_urls=True,\n",
    "        no_emails=True, no_phone_numbers=True,\n",
    "        no_numbers=False, no_digits=False,\n",
    "        no_currency_symbols=False, no_punct=True,\n",
    "        replace_with_punct=\"\",\n",
    "        replace_with_url=\"pagina_web\",\n",
    "        replace_with_email=\"email_usario\",\n",
    "        replace_with_phone_number=\"numero_telefone\",\n",
    "        replace_with_currency_symbol=\"simbolo_monetario\",\n",
    "    )\n",
    "    return text\n",
    "\n",
    "\n",
    "def StopRemover(text, wordslist):\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(wordslist)\n",
    "    return ' '.join([word for word in text.split() if word not in (stopwords)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, stopwords=True, wordlist=[], stemmer=False, lemma=False):\n",
    "        self.stopwords = stopwords\n",
    "        self.wordlist = wordlist\n",
    "        self.stemmer = stemmer\n",
    "        self.lemma = lemma\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X = X.apply(str).apply(lambda text: Normalizer(text=text))\n",
    "\n",
    "        if self.stopwords:\n",
    "            X = X.apply(str).apply(\n",
    "                lambda text: StopRemover(text=text, wordslist=self.wordlist))\n",
    "\n",
    "        if self.lemma:\n",
    "            nlp = spacy.load('pt_core_news_sm')\n",
    "            X = X.apply(str).apply(\n",
    "                lambda x: \" \".join([w.lemma_.lower() for w in nlp(x)]))\n",
    "\n",
    "        if self.stemmer:\n",
    "            X = X.apply(str).apply(PortugueseStemmer().stem)\n",
    "\n",
    "        return X.values.astype('U')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text normalizer\n",
    "wordlist = ['nomeusuario', 'paginaweb', 'emailusario',\n",
    "            'numerotelefone', 'simbolomonetario']\n",
    "\n",
    "normalizer = TextNormalizer(stopwords=True, wordlist=wordlist)\n",
    "\n",
    "# Text vectorizer\n",
    "vectorizer = TfidfVectorizer(lowercase=False,\n",
    "                             analyzer=\"word\",\n",
    "                             norm='l2',\n",
    "                             ngram_range=(1, 2),\n",
    "                             max_features=1500,\n",
    "                             sublinear_tf=True,\n",
    "                             min_df=2)\n",
    "# Classifier\n",
    "classifier = LinearSVC(penalty='l2',\n",
    "                       loss='squared_hinge',\n",
    "                       dual=True,\n",
    "                       tol=1e-6, C=1.1,\n",
    "                       multi_class='crammer_singer',\n",
    "                       fit_intercept=True,\n",
    "                       intercept_scaling=1,\n",
    "                       class_weight=class_weight,\n",
    "                       random_state=42,\n",
    "                       max_iter=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.544304</td>\n",
       "      <td>0.72134</td>\n",
       "      <td>0.696394</td>\n",
       "      <td>0.752456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.721649</td>\n",
       "      <td>0.720670</td>\n",
       "      <td>0.72134</td>\n",
       "      <td>0.721160</td>\n",
       "      <td>0.721340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.779944</td>\n",
       "      <td>0.620192</td>\n",
       "      <td>0.72134</td>\n",
       "      <td>0.700068</td>\n",
       "      <td>0.729511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>388.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>0.72134</td>\n",
       "      <td>567.000000</td>\n",
       "      <td>567.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0           1  accuracy   macro avg  weighted avg\n",
       "precision    0.848485    0.544304   0.72134    0.696394      0.752456\n",
       "recall       0.721649    0.720670   0.72134    0.721160      0.721340\n",
       "f1-score     0.779944    0.620192   0.72134    0.700068      0.729511\n",
       "support    388.000000  179.000000   0.72134  567.000000    567.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a classifier pipeline\n",
    "ml_pipe = Pipeline([('normalizer', normalizer),\n",
    "                    ('vectorizer', vectorizer),\n",
    "                    ('classifier', classifier)])\n",
    "\n",
    "# Train and predict\n",
    "ml_pipe.fit(X_train, y_train)\n",
    "y_pred = ml_pipe.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "pd.DataFrame(classification_report(y_test, y_pred, output_dict=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow tracking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.set_tracking_uri('http://127.0.0.1:5000')\n",
    "# mlflow.set_experiment('Hate Speech')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.metrics import recall_score\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with mlflow.start_run():\n",
    "\n",
    "#     ml_pipe = Pipeline([('vectorizer', TfidfVectorizer(lowercase=False,\n",
    "#                                                        analyzer=\"word\",\n",
    "#                                                        norm='l2',\n",
    "#                                                        ngram_range=(1, 3),\n",
    "#                                                        max_features=100)),\n",
    "#                         ('classifier', DecisionTreeClassifier(random_state=42,\n",
    "#                                                               class_weight={0: 1, 1: 1.5}))])\n",
    "\n",
    "#     ml_pipe.fit(X_train, y_train)\n",
    "#     y_predict = ml_pipe.predict(X_test)\n",
    "    \n",
    "#     mlflow.log_params(ml_pipe.get_params())\n",
    "#     mlflow.log_metric('accuracy', accuracy_score(y_test, y_predict))\n",
    "#     mlflow.log_metric('recall', recall_score(y_test, y_predict))\n",
    "#     mlflow.log_metric('auc', roc_auc_score(y_test, y_predict))\n",
    "#     mlflow.log_metric('f1', f1_score(y_test, y_predict))\n",
    "#     mlflow.sklearn.log_model(ml_pipe, 'DecisionTreeClassifier')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('hate-seepch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7304d3c1a35396b9e299acc644b89f0e56d4154029b20ed6ab08effb23ac070c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
