{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unable warnings\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directory adjustment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Back to main folder\n",
    "path = os.path.dirname(os.getcwd()) + \"/\"\n",
    "os.chdir(path)\n",
    "sys.path.append(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural language processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim import models\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML preprocessing\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.TextVectorization import MeanEmbeddingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Deep learnig model\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import tensorflow as tf\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='./artifacts/1', creation_time=1665929754799, experiment_id='1', last_update_time=1665929754799, lifecycle_stage='active', name='Hate Speech', tags={}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri('http://127.0.0.1:5000')\n",
    "mlflow.set_experiment('Hate Speech')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "df = pd.read_csv(\"data/corpus/augmented_corpus_fortuna.csv\")\n",
    "\n",
    "# Set target and features\n",
    "target = \"label\"\n",
    "features = \"text_nonstop\"\n",
    "count = f\"length_{features}\"\n",
    "pos = len(df.query('label==1'))\n",
    "neg = len(df.query('label==0'))\n",
    "\n",
    "\n",
    "# Break apart dataset\n",
    "X = df[features].values.astype(\"U\")\n",
    "y = df[target]\n",
    "\n",
    "# Split train abd test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Set k-fold criteria\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Corpus considerations\n",
    "corpus = X\n",
    "\n",
    "longest_text = df[count].max()\n",
    "initial_bias = np.log([pos/neg])\n",
    "\n",
    "weight_for_0 = (1 / neg) * (len(df) / 2.0)\n",
    "weight_for_1 = (1 / pos) * (len(df) / 2.0)\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  MLP with Tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.805594</td>\n",
       "      <td>0.520286</td>\n",
       "      <td>0.700176</td>\n",
       "      <td>0.662940</td>\n",
       "      <td>0.715775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.741313</td>\n",
       "      <td>0.610644</td>\n",
       "      <td>0.700176</td>\n",
       "      <td>0.675978</td>\n",
       "      <td>0.700176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.772118</td>\n",
       "      <td>0.561856</td>\n",
       "      <td>0.700176</td>\n",
       "      <td>0.666987</td>\n",
       "      <td>0.705924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>777.000000</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>0.700176</td>\n",
       "      <td>1134.000000</td>\n",
       "      <td>1134.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0           1  accuracy    macro avg  weighted avg\n",
       "precision    0.805594    0.520286  0.700176     0.662940      0.715775\n",
       "recall       0.741313    0.610644  0.700176     0.675978      0.700176\n",
       "f1-score     0.772118    0.561856  0.700176     0.666987      0.705924\n",
       "support    777.000000  357.000000  0.700176  1134.000000   1134.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorizer\n",
    "vectorizer = TfidfVectorizer(lowercase=False, analyzer=\"word\",\n",
    "                             norm='l2', ngram_range=(1, 2), max_features=10000)\n",
    "\n",
    "# Basic pipeline\n",
    "mlflow.sklearn.autolog()\n",
    "with mlflow.start_run():\n",
    "    ml_pipe = Pipeline(\n",
    "        [\n",
    "            (\"vectorizer\", vectorizer),\n",
    "            (\"classifier\", MLPClassifier(\n",
    "                alpha=0.00001,\n",
    "                warm_start=True,\n",
    "                hidden_layer_sizes=(200),\n",
    "                activation='relu',\n",
    "                solver='lbfgs',\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=1000,\n",
    "                max_fun=30000,\n",
    "                random_state=42)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    ml_pipe.fit(X_train, y_train)\n",
    "    y_predict = ml_pipe.predict(X_test)\n",
    "\n",
    "    # Tracking\n",
    "    mlflow.log_params(ml_pipe.get_params())\n",
    "    mlflow.log_metric('precision', precision_score(y_test, y_predict))\n",
    "    mlflow.log_metric('accuracy', accuracy_score(y_test, y_predict))\n",
    "    mlflow.log_metric('recall', recall_score(y_test, y_predict))\n",
    "    mlflow.log_metric('auc', roc_auc_score(y_test, y_predict))\n",
    "    mlflow.log_metric('f1', f1_score(y_test, y_predict))\n",
    "    mlflow.sklearn.log_model(ml_pipe, \"MLP\")\n",
    "\n",
    "# View\n",
    "pd.DataFrame(classification_report(\n",
    "    y_test, ml_pipe.predict(X_test), output_dict=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP with wor2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a corpus\n",
    "corpus = X\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "# Train a own word2vec model\n",
    "my_embedding_50d = gensim.models.Word2Vec(\n",
    "    corpus,\n",
    "    vector_size=50,\n",
    "    window=4,\n",
    "    min_count=10,\n",
    "    sg=1,\n",
    "    workers=cores - 1,\n",
    "    batch_words=10000,\n",
    "    alpha=0.1,\n",
    "    min_alpha=0.0001,\n",
    "    negative=20,\n",
    ")\n",
    "\n",
    "my_embedding_100d = gensim.models.Word2Vec(\n",
    "    corpus,\n",
    "    vector_size=50,\n",
    "    window=4,\n",
    "    min_count=10,\n",
    "    sg=1,\n",
    "    workers=cores - 1,\n",
    "    batch_words=10000,\n",
    "    alpha=0.1,\n",
    "    min_alpha=0.0001,\n",
    "    negative=20,\n",
    ")\n",
    "\n",
    "my_embedding_300d = gensim.models.Word2Vec(\n",
    "    corpus,\n",
    "    vector_size=300,\n",
    "    window=4,\n",
    "    min_count=10,\n",
    "    sg=1,\n",
    "    workers=cores - 1,\n",
    "    batch_words=10000,\n",
    "    alpha=0.1,\n",
    "    min_alpha=0.0001,\n",
    "    negative=20,\n",
    ")\n",
    "\n",
    "\n",
    "# Make embedding dictionary {token:vector}\n",
    "my_embedding_50d = dict(\n",
    "    zip(my_embedding_50d.wv.index_to_key, my_embedding_50d.wv.vectors))\n",
    "\n",
    "my_embedding_100d = dict(\n",
    "    zip(my_embedding_100d.wv.index_to_key, my_embedding_100d.wv.vectors))\n",
    "\n",
    "my_embedding_300d = dict(\n",
    "    zip(my_embedding_300d.wv.index_to_key, my_embedding_300d.wv.vectors))\n",
    "\n",
    "# Embeddings\n",
    "embedding = {\"skip_50\": my_embedding_50d,\n",
    "             \"skip_100\": my_embedding_100d,\n",
    "             \"skip_300\": my_embedding_300d}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "classifier = MLPClassifier(\n",
    "    alpha=0.00001,\n",
    "    hidden_layer_sizes=(200),\n",
    "    activation='relu',\n",
    "    solver='lbfgs',\n",
    "    learning_rate=\"adaptive\",\n",
    "    max_iter=10_000,\n",
    "    max_fun=30_000,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi embedding test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/10/19 11:38:40 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2022-09-16; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'minhasoma'}\n",
      "2022/10/19 11:38:42 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2022-09-16; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'minhasoma'}\n",
      "2022/10/19 11:40:42 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2022-09-16; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'minhasoma'}\n",
      "2022/10/19 11:40:45 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2022-09-16; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'minhasoma'}\n",
      "2022/10/19 11:44:45 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2022-09-16; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'minhasoma'}\n",
      "2022/10/19 11:44:47 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2022-09-16; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'minhasoma'}\n"
     ]
    }
   ],
   "source": [
    "for embedding_name, w2v in embedding.items():\n",
    "    mlflow.sklearn.autolog()\n",
    "    with mlflow.start_run():\n",
    "\n",
    "        ml_pipe = Pipeline([('vectorizer', MeanEmbeddingVectorizer(w2v)),\n",
    "                            ('classifier', classifier)])\n",
    "\n",
    "        # Model fit\n",
    "        ml_pipe.fit(X_train, y_train)\n",
    "        y_predict = ml_pipe.predict(X_test)\n",
    "\n",
    "        # Tracking\n",
    "        mlflow.log_params(ml_pipe.get_params())\n",
    "        mlflow.log_metric('precision', precision_score(y_test, y_predict))\n",
    "        mlflow.log_metric('accuracy', accuracy_score(y_test, y_predict))\n",
    "        mlflow.log_metric('recall', recall_score(y_test, y_predict))\n",
    "        mlflow.log_metric('auc', roc_auc_score(y_test, y_predict))\n",
    "        mlflow.log_metric('f1', f1_score(y_test, y_predict))\n",
    "        mlflow.sklearn.log_model(ml_pipe, embedding_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('hate-seepch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7304d3c1a35396b9e299acc644b89f0e56d4154029b20ed6ab08effb23ac070c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
