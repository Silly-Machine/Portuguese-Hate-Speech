{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directory adjustment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Back to main folder\n",
    "path = os.path.dirname(os.getcwd()) + \"/\"\n",
    "os.chdir(path)\n",
    "sys.path.append(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural language processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunumi/miniconda3/envs/hate-seepch/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim import models\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models\n",
    "[Check](https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-06 14:15:21.612067: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-06 14:15:21.903980: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-06 14:15:21.903995: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-06 14:15:21.935634: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-06 14:15:22.721741: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-06 14:15:22.721820: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-06 14:15:22.721828: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# ML preprocessing\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Deep learnig model\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained word embedding\n",
    "from email import header\n",
    "\n",
    "\n",
    "# word_vectors = KeyedVectors.load_word2vec_format(\n",
    "#     \"data/pretrained-glove/glove.twitter.27B.200d.txt\", binary=False, no_header=True\n",
    "# )\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format(\n",
    "    \"data/pretrained-glove/glove_s300.txt\", binary=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_nonstop</th>\n",
       "      <th>text_lemma</th>\n",
       "      <th>text</th>\n",
       "      <th>length_text_nonstop</th>\n",
       "      <th>length_text_lemma</th>\n",
       "      <th>length_text</th>\n",
       "      <th>label</th>\n",
       "      <th>count_word_text_nonstop</th>\n",
       "      <th>count_word_text_lemma</th>\n",
       "      <th>count_word_text</th>\n",
       "      <th>...</th>\n",
       "      <th>pron</th>\n",
       "      <th>adp</th>\n",
       "      <th>aux</th>\n",
       "      <th>cconj</th>\n",
       "      <th>num</th>\n",
       "      <th>space</th>\n",
       "      <th>intj</th>\n",
       "      <th>sym</th>\n",
       "      <th>punct</th>\n",
       "      <th>part</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cara vive outro mundo nao mundo real refugiado...</td>\n",
       "      <td>caro viver outro mundo nao mundo real refugiad...</td>\n",
       "      <td>nomeusuario o cara vive em outro mundo nao no ...</td>\n",
       "      <td>85</td>\n",
       "      <td>82</td>\n",
       "      <td>124</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>incompetentes nao cuidam povo brasileiro pouco...</td>\n",
       "      <td>incompetente nao cuidar povo brasileiro pouco ...</td>\n",
       "      <td>nomeusuario estes incompetentes nao cuidam nem...</td>\n",
       "      <td>69</td>\n",
       "      <td>66</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text_nonstop  \\\n",
       "0  cara vive outro mundo nao mundo real refugiado...   \n",
       "1  incompetentes nao cuidam povo brasileiro pouco...   \n",
       "\n",
       "                                          text_lemma  \\\n",
       "0  caro viver outro mundo nao mundo real refugiad...   \n",
       "1  incompetente nao cuidar povo brasileiro pouco ...   \n",
       "\n",
       "                                                text  length_text_nonstop  \\\n",
       "0  nomeusuario o cara vive em outro mundo nao no ...                   85   \n",
       "1  nomeusuario estes incompetentes nao cuidam nem...                   69   \n",
       "\n",
       "   length_text_lemma  length_text  label  count_word_text_nonstop  \\\n",
       "0                 82          124      1                       19   \n",
       "1                 66          108      0                       20   \n",
       "\n",
       "   count_word_text_lemma  count_word_text  ...  pron  adp  aux  cconj  num  \\\n",
       "0                     19               20  ...     0    0    0      0    0   \n",
       "1                     20               20  ...     0    0    0      0    0   \n",
       "\n",
       "   space  intj  sym  punct  part  \n",
       "0      0     0    0      0     0  \n",
       "1      0     0    0      0     0  \n",
       "\n",
       "[2 rows x 27 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get data\n",
    "df = pd.read_csv(\"data/corpus/augmented_corpus_fortuna.csv\")\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set target and features\n",
    "target = \"label\"\n",
    "features = \"text_nonstop\"\n",
    "count = f\"length_{features}\"\n",
    "\n",
    "# Break apart dataset\n",
    "X = df[features].values.astype(\"U\")\n",
    "y = df[target]\n",
    "\n",
    "# Split train abd test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Set k-fold criteria\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Corpus considerations\n",
    "corpus = X\n",
    "longest_text = df[count].max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy import array\n",
    "# from numpy import asarray\n",
    "# from numpy import zeros\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras_preprocessing.sequence import pad_sequences\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import Flatten\n",
    "# from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prepare tokenizer\n",
    "# t = Tokenizer()\n",
    "# t.fit_on_texts(corpus)\n",
    "# vocab_size = len(t.word_index) + 1\n",
    "# # integer encode the documents\n",
    "# encoded_docs = t.texts_to_sequences(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pad documents to a max length of 4 words\n",
    "# max_length = 4\n",
    "# padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the whole embedding into memory\n",
    "# embeddings_index = dict()\n",
    "# f = open('data/pretrained-glove/glove.twitter.27B.200d.txt')\n",
    "# for line in f:\n",
    "# \tvalues = line.split()\n",
    "# \tword = values[0]\n",
    "# \tcoefs = asarray(values[1:], dtype='float32')\n",
    "# \tembeddings_index[word] = coefs\n",
    "# f.close()\n",
    "# print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a weight matrix for words in training docs\n",
    "# embedding_matrix = zeros((vocab_size, 200))\n",
    "# for word, i in t.word_index.items():\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define model\n",
    "# model = Sequential()\n",
    "# e = Embedding(\n",
    "#     vocab_size, 200, weights=[embedding_matrix], input_length=4, trainable=False\n",
    "# )\n",
    "# model.add(e)\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(1, activation=\"sigmoid\"))\n",
    "# # compile the model\n",
    "# model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "# # summarize the model\n",
    "# print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fit the model\n",
    "# model.fit(padded_docs, y, epochs=50, verbose=0)\n",
    "# # evaluate the model\n",
    "# loss, accuracy = model.evaluate(padded_docs, y, verbose=0)\n",
    "# print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerTransformer(BaseEstimator, TransformerMixin, Tokenizer):\n",
    "    def __init__(self, **tokenizer_params):\n",
    "        Tokenizer.__init__(self, **tokenizer_params)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.fit_on_texts(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = self.texts_to_sequences(X)\n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "class PadSequencesTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, maxlen):\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_padded = pad_sequences(X, maxlen=self.maxlen)\n",
    "        return X_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(embedding_input_dim, embedding_output_dim, embedding_weights):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Embedding(\n",
    "            input_dim=embedding_input_dim,\n",
    "            output_dim=embedding_output_dim,\n",
    "            weights=[embedding_weights],\n",
    "            trainable=False,\n",
    "            mask_zero=True,\n",
    "        )\n",
    "    )\n",
    "    model.add(LSTM(200))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = np.vstack([\n",
    "           np.zeros(word_vectors.vectors.shape[1]),\n",
    "           word_vectors.vectors\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_vectors) + 1\n",
    "EMBEDDING_DIM = word_vectors.vectors.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65825/1927001707.py:5: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  my_model = KerasClassifier(\n"
     ]
    }
   ],
   "source": [
    "my_tokenizer = TokenizerTransformer()\n",
    "\n",
    "my_padder = PadSequencesTransformer(maxlen=longest_text)\n",
    "\n",
    "my_model = KerasClassifier(\n",
    "    build_fn=create_model,\n",
    "    epochs=50,\n",
    "    embedding_input_dim=vocab_size,\n",
    "    embedding_output_dim=EMBEDDING_DIM,\n",
    "    embedding_weights=embedding_weights,\n",
    "    callbacks=[EarlyStopping(monitor=\"loss\", patience=10)],\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [(\"tokenizer\", my_tokenizer), (\"padder\", my_padder), (\"model\", my_model)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-06 14:17:24.259128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-06 14:17:24.259383: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-06 14:17:24.259452: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-06 14:17:24.259512: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-06 14:17:24.259570: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-10-06 14:17:24.259631: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-10-06 14:17:24.259690: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-06 14:17:24.259748: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-06 14:17:24.259807: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-10-06 14:17:24.259815: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-10-06 14:17:24.260193: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-06 14:17:24.284476: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1115527200 exceeds 10% of free system memory.\n",
      "2022-10-06 14:17:24.555514: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1115527200 exceeds 10% of free system memory.\n",
      "2022-10-06 14:17:24.730031: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1115527200 exceeds 10% of free system memory.\n",
      "2022-10-06 14:17:25.675738: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1115527200 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "142/142 [==============================] - 18s 107ms/step - loss: 0.6117 - accuracy: 0.6951\n",
      "Epoch 2/50\n",
      "142/142 [==============================] - 15s 103ms/step - loss: 0.5545 - accuracy: 0.7220\n",
      "Epoch 3/50\n",
      "142/142 [==============================] - 15s 104ms/step - loss: 0.5041 - accuracy: 0.7551\n",
      "Epoch 4/50\n",
      "142/142 [==============================] - 15s 104ms/step - loss: 0.4404 - accuracy: 0.7963\n",
      "Epoch 5/50\n",
      "142/142 [==============================] - 15s 106ms/step - loss: 0.3548 - accuracy: 0.8455\n",
      "Epoch 6/50\n",
      "142/142 [==============================] - 15s 105ms/step - loss: 0.2358 - accuracy: 0.9074\n",
      "Epoch 7/50\n",
      "142/142 [==============================] - 15s 107ms/step - loss: 0.1372 - accuracy: 0.9559\n",
      "Epoch 8/50\n",
      "142/142 [==============================] - 15s 106ms/step - loss: 0.0846 - accuracy: 0.9777\n",
      "Epoch 9/50\n",
      "142/142 [==============================] - 15s 106ms/step - loss: 0.0598 - accuracy: 0.9854\n",
      "Epoch 10/50\n",
      "142/142 [==============================] - 15s 107ms/step - loss: 0.0459 - accuracy: 0.9879\n",
      "Epoch 11/50\n",
      "142/142 [==============================] - 15s 107ms/step - loss: 0.0324 - accuracy: 0.9910\n",
      "Epoch 12/50\n",
      "142/142 [==============================] - 15s 108ms/step - loss: 0.0289 - accuracy: 0.9910\n",
      "Epoch 13/50\n",
      "142/142 [==============================] - 15s 106ms/step - loss: 0.0238 - accuracy: 0.9901\n",
      "Epoch 14/50\n",
      "142/142 [==============================] - 15s 106ms/step - loss: 0.0198 - accuracy: 0.9916\n",
      "Epoch 15/50\n",
      "142/142 [==============================] - 15s 107ms/step - loss: 0.0192 - accuracy: 0.9912\n",
      "Epoch 16/50\n",
      "142/142 [==============================] - 15s 107ms/step - loss: 0.0180 - accuracy: 0.9901\n",
      "Epoch 17/50\n",
      "142/142 [==============================] - 15s 106ms/step - loss: 0.0339 - accuracy: 0.9857\n",
      "Epoch 18/50\n",
      "142/142 [==============================] - 15s 106ms/step - loss: 0.0552 - accuracy: 0.9793\n",
      "Epoch 19/50\n",
      "142/142 [==============================] - 15s 107ms/step - loss: 0.0391 - accuracy: 0.9859\n",
      "Epoch 20/50\n",
      "142/142 [==============================] - 15s 106ms/step - loss: 0.0240 - accuracy: 0.9885\n",
      "Epoch 21/50\n",
      "142/142 [==============================] - 15s 106ms/step - loss: 0.0182 - accuracy: 0.9916\n",
      "Epoch 22/50\n",
      "142/142 [==============================] - 15s 107ms/step - loss: 0.0157 - accuracy: 0.9914\n",
      "Epoch 23/50\n",
      "142/142 [==============================] - 15s 106ms/step - loss: 0.0150 - accuracy: 0.9912\n",
      "Epoch 24/50\n",
      "142/142 [==============================] - 15s 107ms/step - loss: 0.0147 - accuracy: 0.9910\n",
      "Epoch 25/50\n",
      "142/142 [==============================] - 15s 106ms/step - loss: 0.0140 - accuracy: 0.9910\n",
      "Epoch 26/50\n",
      "142/142 [==============================] - 15s 106ms/step - loss: 0.0138 - accuracy: 0.9910\n",
      "Epoch 27/50\n",
      "142/142 [==============================] - 15s 108ms/step - loss: 0.0134 - accuracy: 0.9921\n",
      "Epoch 28/50\n",
      "142/142 [==============================] - 15s 106ms/step - loss: 0.0128 - accuracy: 0.9910\n",
      "Epoch 29/50\n",
      "142/142 [==============================] - 15s 108ms/step - loss: 0.0127 - accuracy: 0.9918\n",
      "Epoch 30/50\n",
      "142/142 [==============================] - 15s 106ms/step - loss: 0.0126 - accuracy: 0.9912\n",
      "Epoch 31/50\n",
      "142/142 [==============================] - 15s 106ms/step - loss: 0.0121 - accuracy: 0.9921\n",
      "Epoch 32/50\n",
      "142/142 [==============================] - 15s 106ms/step - loss: 0.0123 - accuracy: 0.9912\n",
      "Epoch 33/50\n",
      "142/142 [==============================] - 15s 106ms/step - loss: 0.0125 - accuracy: 0.9903\n",
      "Epoch 34/50\n",
      "142/142 [==============================] - 15s 106ms/step - loss: 0.0121 - accuracy: 0.9905\n",
      "Epoch 35/50\n",
      "142/142 [==============================] - 15s 108ms/step - loss: 0.0117 - accuracy: 0.9905\n",
      "Epoch 36/50\n",
      "142/142 [==============================] - 16s 110ms/step - loss: 0.0118 - accuracy: 0.9907\n",
      "Epoch 37/50\n",
      "142/142 [==============================] - 15s 107ms/step - loss: 0.0114 - accuracy: 0.9912\n",
      "Epoch 38/50\n",
      "142/142 [==============================] - 15s 106ms/step - loss: 0.0922 - accuracy: 0.9652\n",
      "Epoch 39/50\n",
      "142/142 [==============================] - 15s 108ms/step - loss: 0.0403 - accuracy: 0.9846\n",
      "Epoch 40/50\n",
      "142/142 [==============================] - 15s 106ms/step - loss: 0.0178 - accuracy: 0.9905\n",
      "Epoch 41/50\n",
      "142/142 [==============================] - 15s 106ms/step - loss: 0.0137 - accuracy: 0.9901\n",
      "Epoch 42/50\n",
      "142/142 [==============================] - 15s 106ms/step - loss: 0.0126 - accuracy: 0.9912\n",
      "Epoch 43/50\n",
      "142/142 [==============================] - 15s 107ms/step - loss: 0.0119 - accuracy: 0.9912\n",
      "Epoch 44/50\n",
      "142/142 [==============================] - 15s 107ms/step - loss: 0.0117 - accuracy: 0.9910\n",
      "Epoch 45/50\n",
      "142/142 [==============================] - 15s 106ms/step - loss: 0.0118 - accuracy: 0.9905\n",
      "Epoch 46/50\n",
      "142/142 [==============================] - 15s 108ms/step - loss: 0.0116 - accuracy: 0.9907\n",
      "Epoch 47/50\n",
      "142/142 [==============================] - 15s 106ms/step - loss: 0.0115 - accuracy: 0.9905\n",
      "36/36 [==============================] - 2s 39ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.806950</td>\n",
       "      <td>0.782772</td>\n",
       "      <td>777.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.514563</td>\n",
       "      <td>0.445378</td>\n",
       "      <td>0.477477</td>\n",
       "      <td>357.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.693122</td>\n",
       "      <td>0.693122</td>\n",
       "      <td>0.693122</td>\n",
       "      <td>0.693122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.637282</td>\n",
       "      <td>0.626164</td>\n",
       "      <td>0.630125</td>\n",
       "      <td>1134.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.682733</td>\n",
       "      <td>0.693122</td>\n",
       "      <td>0.686660</td>\n",
       "      <td>1134.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0              0.760000  0.806950  0.782772   777.000000\n",
       "1              0.514563  0.445378  0.477477   357.000000\n",
       "accuracy       0.693122  0.693122  0.693122     0.693122\n",
       "macro avg      0.637282  0.626164  0.630125  1134.000000\n",
       "weighted avg   0.682733  0.693122  0.686660  1134.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "pd.DataFrame(\n",
    "    classification_report(y_test, pipeline.predict(X_test), output_dict=True)\n",
    ").T\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('hate-seepch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7304d3c1a35396b9e299acc644b89f0e56d4154029b20ed6ab08effb23ac070c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
